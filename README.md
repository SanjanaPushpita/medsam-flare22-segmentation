# MedSAM Tumor Segmentation on FLARE22

Single-model liver (focal organ) segmentation pipeline based on **MedSAM** fine-tuning. The repo captures the final Colab workflow, cached metrics, and qualitative outputs used to generate the results shown in the notebook.


## Repository Contents

```
tumor_segmentation/
├── README.md
├── LICENSE
├── medsam_vit_b.pth                # MedSAM checkpoint (included for convenience)
├── notebooks/
│   └── final_tumor_segmentation.ipynb
├── src/
│   ├── __init__.py
│   ├── configs.py
│   ├── data.py
│   ├── medsam_wrapper.py
│   ├── metrics.py
│   ├── plotting.py
│   └── trainers.py                 # contains legacy trainers (disabled by default)
├── CSV/                            # exported metrics
│   ├── medsam_classification_report.csv
│   └── metrics_summary.csv
├── npy/                            # saved confusion matrix
│   └── medsam_confusion_matrix.npy
├── png/                            # visualizations (training curves, overlays, etc.)
└── cache/                          # cached slice index / preview assets
```

## Dataset

- **Source:** [MICCAI FLARE22 Challenge](https://flare22.grand-challenge.org/) – 50 labeled abdomen CT scans.
- **Usage:** Download the training set and place it in Google Drive (`MyDrive/MICCAI FLARE22 Challenge Dataset (50 Labeled Abdomen CT Scans)/FLARE22Train/{images,labels}`). Respect the dataset license and challenge terms.

## Notebook Workflow (Google Colab)

Open `notebooks/final_tumor_segmentation.ipynb` in Colab and run sequentially:

1. **Environment setup** – installs dependencies (PyTorch, MONAI, SAM) and verifies GPU availability.
2. **Path configuration** – points `CONFIG` to your Drive dataset/weights and applies optional runtime overrides.
3. **Data preparation** – builds MONAI dataloaders plus a cached positive-slice index for MedSAM.
4. **Training** – fine-tunes MedSAM on axial slices (default quick-run: 30 epochs, batch size 2 for MedSAM).
5. **Evaluation** – computes Dice/IoU/accuracy and exports reports to `CSV/` & `npy/`.
6. **Visualization** – renders confusion matrix and slice overlays (CT vs. ground truth vs. MedSAM prediction).

### Quick-Run vs Full Training

- `CONFIG.quick_run = True` (default): uses a reduced subset (8 volumes) for rapid iteration on Colab GPUs.
- Set `runtime_overrides["quick_run"] = False` and adjust epochs/batch sizes for full training (requires more VRAM and time).

## Results Snapshot

- **Validation Dice:** ≈ **0.84**
- **Validation Accuracy:** ≈ **0.98**
- Confusion matrix and classification report stored in `CSV/` and `npy/` directories; matching overlay saved under `png/`.

## Notes

- Only the MedSAM model is currently executed; legacy trainer classes remain in `src/trainers.py` for reference but are not invoked.
- `medsam_vit_b.pth` (≈360 MB) is bundled so the notebook can run offline. If you prefer, you can delete it before committing and download from the [MedSAM release](https://github.com/bowang-lab/MedSAM) during runtime.
- Cached artifacts (`cache/`, `CSV/`, `png/`, `npy/`) demonstrate the final run and can be regenerated by rerunning the notebook.

## Getting Started Locally (Optional)

If you want to run outside Colab, install dependencies in a Python ≥3.10 environment, adjust the path overrides in the notebook, and execute cells using JupyterLab or VS Code. GPU acceleration is strongly recommended.

## Citation

Please cite the original resources if you build upon this work:

```
@inproceedings{ji2022flare22,
  title     = {FLARE22 Challenge: Fast and Low-resource Abdomen multi-organ segmentation},
  author    = {Ji, Y. and others},
  year      = {2022}
}

@article{huang2023medsam,
  title     = {MedSAM: Segment Anything in Medical Images},
  author    = {Huang, X. and others},
  year      = {2023},
  note      = {arXiv:2304.12306}
}
```

---

**Maintainer:** Maliha Sanjana (project author)

